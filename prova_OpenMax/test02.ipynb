{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from avalanche.benchmarks.datasets import MNIST\n",
    "from avalanche.benchmarks.datasets.dataset_utils import default_dataset_location\n",
    "from avalanche.benchmarks.utils import as_classification_dataset, AvalancheDataset\n",
    "\n",
    "\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.benchmarks.generators import nc_benchmark\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training import Naive\n",
    "from avalanche.training.plugins import (\n",
    "    ReplayPlugin,\n",
    "    EWCPlugin,\n",
    "    AGEMPlugin,\n",
    "    EvaluationPlugin,\n",
    ")\n",
    "from avalanche.evaluation.metrics import (\n",
    "    forgetting_metrics,\n",
    "    accuracy_metrics,\n",
    "    loss_metrics,\n",
    "    timing_metrics,\n",
    "    cpu_usage_metrics,\n",
    "    confusion_matrix_metrics,\n",
    "    disk_usage_metrics,\n",
    ")\n",
    "from avalanche.logging import InteractiveLogger\n",
    "\n",
    "from pytorch_ood.detector import OpenMax, EnergyBased, Entropy\n",
    "from pytorch_ood.utils import OODMetrics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from pytorch_ood.model import WideResNet\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from avalanche.benchmarks.datasets import MNIST\n",
    "from avalanche.benchmarks.datasets.dataset_utils import default_dataset_location\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from pytorch_ood.utils import OODMetrics\n",
    "from pytorch_ood.dataset.img import Textures\n",
    "from pytorch_ood.utils import ToUnknown\n",
    "from pytorch_ood.model import WideResNet\n",
    "from avalanche.models import SimpleMLP\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0, 10}, {1, 11}, {2, 12}, {3, 13}, {4, 14}, {5, 15}, {16, 6}, {17, 7}, {8, 18}, {9, 19}]\n",
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]\n",
      "Train DataLoader and Test DataLoader have been created successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "\n",
    "def remove_channel_dimension(dataset):\n",
    "    modified_data = []\n",
    "    for data in dataset:\n",
    "        modified_data.append(data.squeeze(-1))  # Remove the last channel dimension\n",
    "    return torch.stack(modified_data)  # Stack the modified data and return as a tensor\n",
    "\n",
    "\n",
    "def load_data(batch_size=64, seed=0):\n",
    "    # Location to save/load the MNIST dataset\n",
    "    datadir = default_dataset_location(\"mnist\")\n",
    "\n",
    "    # Load the non-corrupted MNIST dataset\n",
    "    train_MNIST = MNIST(datadir, train=True, download=True)\n",
    "    test_MNIST = MNIST(datadir, train=False, download=True)\n",
    "\n",
    "    # Extract train and test data/labels\n",
    "    train_data = train_MNIST.data.float() / 255  # Normalize data to [0, 1]\n",
    "    train_labels = train_MNIST.targets\n",
    "    test_data = test_MNIST.data.float() / 255  # Normalize data to [0, 1]\n",
    "    test_labels = test_MNIST.targets\n",
    "\n",
    "    # Load corrupted data and labels\n",
    "    c_test_images = (\n",
    "        np.load(\"./brightness/test/test_images.npy\").astype(np.float32) / 255\n",
    "    )  # Normalize\n",
    "    c_test_labels = np.load(\"./brightness/test/test_labels.npy\")\n",
    "    # c_train_images = (\n",
    "    #     np.load(\"./brightness/train/train_images.npy\").astype(np.float32) / 255\n",
    "    # )  # Normalize\n",
    "    # c_train_labels = np.load(\"./brightness/train/train_labels.npy\")\n",
    "\n",
    "    # Convert NumPy arrays to tensors and remove channel dimension for images\n",
    "    c_test_images_tensor = remove_channel_dimension(torch.tensor(c_test_images))\n",
    "    # c_train_images_tensor = remove_channel_dimension(torch.tensor(c_train_images))\n",
    "\n",
    "    # Apply the specified mapping to the corrupted labels\n",
    "    def map_labels(labels):\n",
    "        return torch.tensor([10 if label == 0 else label+10 for label in labels])\n",
    "\n",
    "    c_test_labels_tensor = map_labels(c_test_labels)\n",
    "    # c_train_labels_tensor = map_labels(c_train_labels)\n",
    "\n",
    "    # Combine non-corrupted and corrupted data\n",
    "    combined_test_data = torch.cat(\n",
    "        [test_data, c_test_images_tensor], dim=0\n",
    "    )  # Add channel dimension\n",
    "    combined_test_labels = torch.cat([test_labels, c_test_labels_tensor], dim=0)\n",
    "    \n",
    "    # combined_train_data = torch.cat(\n",
    "    #     [train_data, c_train_images_tensor], dim=0\n",
    "    # )  # Add channel dimension\n",
    "    # combined_train_labels = torch.cat([train_labels, c_train_labels_tensor], dim=0)\n",
    "\n",
    "    # Create TensorDataset objects\n",
    "    # train_dataset = TensorDataset(combined_train_data, combined_train_labels)\n",
    "    \n",
    "    combined_test_dataset = TensorDataset(combined_test_data, combined_test_labels)\n",
    "\n",
    "    # Create a train Dataset\n",
    "    train_dataset = TensorDataset(train_data, train_labels)\n",
    "    # Create DataLoader objects\n",
    "    train_dataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_dataLoader = DataLoader(\n",
    "        combined_test_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # desired_order = [0, -10, 1, -1, 2, -2, 3, -3, 4, -4, 5, -5, 6, -6, 7, -7, 8, -8, 9, -9]\n",
    "    \n",
    "    desired_order = [0, 10, 1, 11, 2, 12, 3, 13, 4, 14, 5, 15, 6, 16, 7, 17, 8, 18, 9, 19]\n",
    "\n",
    "    scenario = nc_benchmark(\n",
    "        combined_test_dataset,\n",
    "        combined_test_dataset,\n",
    "        n_experiences=10,\n",
    "        shuffle=False,\n",
    "        seed=seed,\n",
    "        fixed_class_order=desired_order,\n",
    "        task_labels=True,\n",
    "    )\n",
    "\n",
    "    return train_dataLoader, test_dataLoader, scenario \n",
    "\n",
    "\n",
    "# Example usage\n",
    "batch_size = 64\n",
    "train_dataLoader, test_dataLoader, scenario= load_data(batch_size=batch_size)\n",
    "\n",
    "# labels_tensor = test_dataLoader.dataset.tensors[1]\n",
    "\n",
    "# # Count the number of occurrences of each class\n",
    "# unique_classes, counts = torch.unique(labels_tensor, return_counts=True)\n",
    "\n",
    "# # Print the counts for each class\n",
    "# for class_index, count in zip(unique_classes, counts):\n",
    "#     print(f\"Class {class_index.item()}: {count.item()} samples\")\n",
    "print(scenario.original_classes_in_exp)\n",
    "print(scenario.task_labels)\n",
    "print(\"Train DataLoader and Test DataLoader have been created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(\n",
    "            64, 10\n",
    "        )  # Output size matches the number of desired classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input images\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def train_model(train_loader):\n",
    "    # model = SimpleNN()\n",
    "    model = SimpleMLP()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # # Print average loss per epoch\n",
    "        # print(\n",
    "        #     f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\"\n",
    "        # )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0, 10}, {1, 11}, {2, 12}, {3, 13}, {4, 14}, {5, 15}, {16, 6}, {17, 7}, {8, 18}, {9, 19}]\n",
      "[0, 10]\n",
      "{'AUROC': 0.9558725357055664, 'AUPR-IN': 0.9455128908157349, 'AUPR-OUT': 0.9594336748123169, 'FPR95TPR': 0.19081632792949677}\n",
      "[1, 11]\n",
      "{'AUROC': 0.7978178858757019, 'AUPR-IN': 0.848194420337677, 'AUPR-OUT': 0.7332760691642761, 'FPR95TPR': 0.8440528512001038}\n",
      "[2, 12]\n",
      "{'AUROC': 0.8337783813476562, 'AUPR-IN': 0.8581138849258423, 'AUPR-OUT': 0.8255239725112915, 'FPR95TPR': 0.6346899271011353}\n",
      "[3, 13]\n",
      "{'AUROC': 0.8885226249694824, 'AUPR-IN': 0.9102345705032349, 'AUPR-OUT': 0.8757404088973999, 'FPR95TPR': 0.5267326831817627}\n",
      "[4, 14]\n",
      "{'AUROC': 0.7842042446136475, 'AUPR-IN': 0.7521378397941589, 'AUPR-OUT': 0.8108094334602356, 'FPR95TPR': 0.5997963547706604}\n",
      "[5, 15]\n",
      "{'AUROC': 0.8753204941749573, 'AUPR-IN': 0.8914667963981628, 'AUPR-OUT': 0.8674235343933105, 'FPR95TPR': 0.5728699564933777}\n",
      "[16, 6]\n",
      "{'AUROC': 0.8973678350448608, 'AUPR-IN': 0.8991154432296753, 'AUPR-OUT': 0.8994457721710205, 'FPR95TPR': 0.424843430519104}\n",
      "[17, 7]\n",
      "{'AUROC': 0.9057669639587402, 'AUPR-IN': 0.9168709516525269, 'AUPR-OUT': 0.8983490467071533, 'FPR95TPR': 0.4776264727115631}\n",
      "[8, 18]\n",
      "{'AUROC': 0.7240101099014282, 'AUPR-IN': 0.7886205911636353, 'AUPR-OUT': 0.6473868489265442, 'FPR95TPR': 0.9106776118278503}\n",
      "[9, 19]\n",
      "{'AUROC': 0.6943838596343994, 'AUPR-IN': 0.749503493309021, 'AUPR-OUT': 0.6256939172744751, 'FPR95TPR': 0.9256689548492432}\n",
      "{'AUROC': 0.8411320447921753, 'AUPR-IN': 0.8631280064582825, 'AUPR-OUT': 0.8341423869132996, 'FPR95TPR': 0.6269000172615051}\n"
     ]
    }
   ],
   "source": [
    "def map_test(labels):\n",
    "    return torch.tensor([-1 if label > 9 else label for label in labels])\n",
    "\n",
    "\n",
    "test_stream = scenario.test_stream\n",
    "\n",
    "large_metrics = OODMetrics()\n",
    "model = train_model(train_dataLoader)\n",
    "detector = OpenMax(model, tailsize=25, alpha=5, euclid_weight=0.5)\n",
    "detector.fit(train_dataLoader)\n",
    "\n",
    "print(scenario.original_classes_in_exp)\n",
    "\n",
    "for exp in scenario.train_stream:\n",
    "    print(exp.classes_in_this_experience)\n",
    "    test_loader = DataLoader(exp.dataset, batch_size=128, shuffle=True)\n",
    "    narrow_metrics = OODMetrics()\n",
    "    for batch in test_loader:\n",
    "        if len(batch) == 2:\n",
    "            x, y = batch\n",
    "        else:\n",
    "            x, y, *_ = batch\n",
    "        y = map_test(y)\n",
    "        large_metrics.update(detector(x),y)\n",
    "        narrow_metrics.update(detector(x),y)\n",
    "    print(narrow_metrics.compute())\n",
    "print(large_metrics.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
